import os
import sys
import time

# ì™¸ë¶€ ë¼ì´ë¸ŒëŸ¬ë¦¬
import streamlit as st
from sentence_transformers import SentenceTransformer
from chromadb import PersistentClient

# sys.path ì¶”ê°€(OpenA1\Main í´ë”)
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..', 'Main')))

# ë‚´ë¶€ ëª¨ë“ˆ
## ì‚¬ì´ë“œë°”
from sidebar import render_history_sidebar, LABELS

## FileSearch ëª¨ë“ˆ
from file_search_utils import file_search_query, create_file, create_vector_store

## ìœ í˜• ë¶„ë¥˜ ëª¨ë¸ ê´€ë ¨ ëª¨ë“ˆ
from model_Frontend_v3 import classify_legal_issue, load_model

## UIë Œë” ê´€ë ¨ ëª¨ë“ˆ
from ui_components import (
    render_user_input,
    render_user_message,
    render_assistant_message,
    render_assistant_message_stream,
    render_title_image,
    render_law_info,
)

## GPT(AI) ì—°ë™ ê´€ë ¨ ëª¨ë“ˆ
from action_guide import action_guide_agent
from law_info_ExceptDB import get_law_info
from openai_utils import get_openai_client

# JSON í•„í„°ë§ ìœ í‹¸ - ì´ì œ filter_casesëŠ” ì•ˆ ì”€, load_casesëŠ” ì¼€ì´ìŠ¤ ì°¸ê³ ìš©ì´ë¼ ë†”ë‘ 
from json_filtering import load_cases  

# Client ì„¤ì •
client = get_openai_client()

################################################################################
######################### ë²¡í„° DB ë° ì„ë² ë”© ëª¨ë¸ ë¡œë“œ #########################
################################################################################

@st.cache_resource
def load_vector_db():
    db_path = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', 'chromadb'))
    client = PersistentClient(path=db_path)
    collection = client.get_collection(name="privacy_cases")
    embed_model = SentenceTransformer("snunlp/KR-SBERT-V40K-klueNLI-augSTS")
    return collection, embed_model

def filter_cases_vector_db(user_input, collection, embed_model, top_n=1):
    query_emb = embed_model.encode([user_input])
    results = collection.query(query_embeddings=query_emb, n_results=top_n)
    cases = results["documents"][0]
    metadatas = results["metadatas"][0]
    return list(zip(cases, metadatas))

################################################################################
################################ Page Config ###################################
################################################################################

st.set_page_config(
    page_title="Streamly - An Intelligent Streamlit Assistant",
    page_icon="imgs/avatar_streamly.png",
    layout="wide",
    initial_sidebar_state="auto",
    menu_items={
        "Get Help": "https://github.com/khj200013/OpenA1/issues",
        "Report a bug": "https://github.com/khj200013/OpenA1/issues",
        "About": """
            ## AIë¥¼ í†µí•´ ê°œì¸ì •ë³´ ë³´í˜¸ë²•ì— ìœ„ë°°ë˜ëŠ” ì‚¬í•­ í™•ì¸
            ### Powered using GPT-4

            **GitHub**: https://github.com/khj200013/OpenA1
            **ë¬¸ì˜ ë˜ëŠ” ë²„ê·¸ ì œë³´**: [Issue ë“±ë¡í•˜ê¸°](https://github.com/khj200013/OpenA1/issues)

            ì´ AI ì–´ì‹œìŠ¤í„´íŠ¸ëŠ” ì‚¬ìš©ìì˜ ìƒí™©ì´ë‚˜ ì§ˆë¬¸ì„ ë°”íƒ•ìœ¼ë¡œ  
            **ê°œì¸ì •ë³´ë³´í˜¸ë²• ë˜ëŠ” ê´€ë ¨ ê·œì •(GDPR ë“±)**ì— ë”°ë¼  
            ìœ„ë°˜ ê°€ëŠ¥ì„±ì„ ë¶„ì„í•˜ê³ , ê´€ë ¨ ë²•ë ¹ê³¼ ëŒ€ì‘ ë°©ì•ˆì„ ì•ˆë‚´í•©ë‹ˆë‹¤.

            Image created by OpenAI
        """
    }
)

################################################################################
################################ GPT UI êµ¬ì„± ###################################
################################################################################

# ë¡œê³  ì´ë¯¸ì§€ ì¶œë ¥
render_title_image()

# íˆìŠ¤í† ë¦¬ìš© ë”•ì…”ë„ˆë¦¬
if "history" not in st.session_state:
    st.session_state.history = { label: [] for label in LABELS }

# ê¸°ì¡´ json ì¼€ì´ìŠ¤ ë°ì´í„° ë¡œë“œ (ì°¸ê³ ìš©)
cases = load_cases()

# ëª¨ë¸ ë¡œë”©
if "model_loaded" not in st.session_state:
    with st.spinner("ëª¨ë¸ ë° ë²¡í„° ìŠ¤í† ì–´ë¥¼ ì´ˆê¸°í™”í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤...ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”.."):
        tokenizer, model = load_model()
        st.session_state.tokenizer = tokenizer
        st.session_state.model = model
        st.session_state.model_loaded = True

        # File Searchìš© ì„¤ì •
        file_path = "./ê°œì¸ì •ë³´ë³´í˜¸ë²•.pdf"
        file_id = create_file(client, file_path)
        vector_store_id = create_vector_store(client, file_id)

        st.session_state.file_id = file_id
        st.session_state.vector_store = vector_store_id

if 'openai_model' not in st.session_state:
    st.session_state.openai_model = 'gpt-4.1'

if 'messages' not in st.session_state:
    st.session_state.messages = []

# ë²¡í„° DB ë° ì„ë² ë”© ëª¨ë¸ì€ í•œ ë²ˆë§Œ ë¡œë“œí•´ ì„¸ì…˜ì— ì €ì¥
if "vector_db_loaded" not in st.session_state:
    collection, embed_model = load_vector_db()
    st.session_state.collection = collection
    st.session_state.embed_model = embed_model
    st.session_state.vector_db_loaded = True

# ê¸°ì¡´ ë©”ì‹œì§€ ì¶œë ¥
for i, msg in enumerate(st.session_state.messages):
    if msg["role"] == "system":
        continue  
    if msg["role"] == 'user':
        render_user_message(msg["content"])
    elif  msg["role"] == 'assistant': 
        if msg.get("law_info"):
            render_law_info(msg["law_info"], msg.get("file_search_result"))
        render_assistant_message(msg["predicted_label"], msg["content"])

# ì‚¬ìš©ì ì…ë ¥ ì €ì¥ í›„ rerun ì²˜ë¦¬ (stream ì¤‘ ì…ë ¥ ë°©ì§€)
if "user_input" not in st.session_state:
    if prompt := render_user_input():
        st.session_state.user_input = prompt
        st.rerun()

# GPT ì‘ë‹µ ì²˜ë¦¬
if "user_input" in st.session_state:
    prompt = st.session_state.user_input
    # ì‚¬ìš©ì ë©”ì‹œì§€ session_stateì— ì¶”ê°€
    st.session_state.messages.append({"role": "user", "content": prompt})

    # ìœ ì € ë©”ì‹œì§€ ë Œë”
    render_user_message(prompt)

    # -------------------------------------------
    # 1ì°¨ ë²¡í„° DB í•„í„°ë§ ì‹œë„
    matched_results = filter_cases_vector_db(prompt, st.session_state.collection, st.session_state.embed_model)

    if matched_results:
        matched_case, metadata = matched_results[0]
        predicted_label = metadata.get("label", "ì•Œ ìˆ˜ ì—†ìŒ")
        law_article = metadata.get("law_article", "")
        st.write(f"ë²¡í„° DB ë§¤ì¹­ ì‚¬ë¡€:\n{matched_case}\në¼ë²¨: {predicted_label}")
    else:
        # ë²¡í„° ê²€ìƒ‰ ì‹¤íŒ¨ ì‹œ ê¸°ì¡´ ë¶„ë¥˜ + GPT ì²˜ë¦¬
        st.write("ë²¡í„° DBì—ì„œ ë§¤ì¹­ ì‹¤íŒ¨, ê¸°ì¡´ ë¶„ë¥˜ê¸° ì‚¬ìš©")
        predicted_label = classify_legal_issue(prompt, st.session_state.tokenizer, st.session_state.model)
        law_article = None

    # GPT PROMPT
    with st.spinner("ğŸ” ë²•ë¥  ì •ë³´ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤..."):
        law_info = get_law_info(predicted_label, prompt)

        # File Search
        file_search_result = file_search_query(client, law_info['law'], st.session_state.vector_store)
        
    # Law info + File search ê°’ ì¶œë ¥
    render_law_info(law_info, file_search_result)

    response_stream = action_guide_agent(prompt, predicted_label)

    # AI ë©”ì‹œì§€ ë Œë” (Stream í˜•íƒœ)
    response_text = render_assistant_message_stream(predicted_label, response_stream)
    
    # AI ë©”ì‹œì§€ session_stateì— ì¶”ê°€
    st.session_state.messages.append({
        "role": "assistant", 
        "predicted_label": predicted_label, 
        "law_info": law_info,
        "file_search_result": file_search_result,
        "content": response_text
    })

    # ì§ˆë¬¸ + ë‹µë³€ ì €ì¥
    st.session_state.history[predicted_label].append({
        "question": prompt,
        "answer": response_text
    })
    
    # user_input ì‚­ì œ í›„ rerun
    del st.session_state.user_input
    st.rerun()

################################################################################
################################[ ì‚¬ì´ë“œë°” ]###################################
################################################################################

render_history_sidebar()

