import streamlit as st
from openai import OpenAI
from dotenv import load_dotenv
import torch, os
from transformers import BertTokenizer, BertForSequenceClassification

# 1. ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¶ˆëŸ¬ì˜¤ê¸°
@st.cache_resource  # ìºì‹±í•´ì„œ ì¬ì‹¤í–‰ ì‹œ ë¹ ë¥´ê²Œ
def load_model():
    # tokenizer = BertTokenizer.from_pretrained("./saved_klue_bert1")
    # model = BertForSequenceClassification.from_pretrained("./saved_klue_bert1")
    tokenizer = BertTokenizer.from_pretrained("../../saved_klue_bert")
    model = BertForSequenceClassification.from_pretrained("../../saved_klue_bert")
    model.eval()
    return tokenizer, model

tokenizer, model = load_model()

label_map = {
    0: "ì´ˆìƒê¶Œ ì¹¨í•´",
    1: "ë™ì˜ ì—†ëŠ” ê°œì¸ì •ë³´ ìˆ˜ì§‘",
    2: "ëª©ì  ì™¸ ì´ìš©",
    3: "ì œ3ì ë¬´ë‹¨ ì œê³µ",
    4: "CCTV ê³¼ì‰ì´¬ì˜",
    5: "ì •ë³´ ìœ ì¶œ",
    6: "íŒŒê¸° ë¯¸ì´í–‰",
    7: "ê´‘ê³ ì„± ì •ë³´ ìˆ˜ì‹ ",
    8: "ê³„ì •/ë¹„ë°€ë²ˆí˜¸ ê´€ë ¨ ë¬¸ì œ"
}

# 2. ë¶„ë¥˜ í•¨ìˆ˜
def classify_legal_issue(text):
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=64)
    with torch.no_grad():
        outputs = model(**inputs)
    pred = torch.argmax(outputs.logits, dim=1).item()
    return label_map[pred]

# í”¼ë“œë°± í•¨ìˆ˜
def save_feedback(index):
    st.session_state.messages[index]["feedback"] = st.session_state[f"feedback_{index}"]


################################################################################
################################ Page Config ###################################
################################################################################


# Page Config êµ¬ì„±
st.set_page_config(
    page_title="Streamly - An Intelligent Streamlit Assistant",
    page_icon="imgs/avatar_streamly.png",
    layout="wide",
    initial_sidebar_state="auto",
    menu_items={
        "Get Help": "https://github.com/khj200013/OpenA1/issues",
        "Report a bug": "https://github.com/khj200013/OpenA1/issues",
        "About": """
            ## AIë¥¼ í†µí•´ ê°œì¸ì •ë³´ ë³´í˜¸ë²•ì— ìœ„ë°°ë˜ëŠ” ì‚¬í•­ í™•ì¸
            ### Powered using GPT-4

            **GitHub**: https://github.com/khj200013/OpenA1
            **ë¬¸ì˜ ë˜ëŠ” ë²„ê·¸ ì œë³´**: [Issue ë“±ë¡í•˜ê¸°](https://github.com/khj200013/OpenA1/issues)

            ì´ AI ì–´ì‹œìŠ¤í„´íŠ¸ëŠ” ì‚¬ìš©ìì˜ ìƒí™©ì´ë‚˜ ì§ˆë¬¸ì„ ë°”íƒ•ìœ¼ë¡œ  
            **ê°œì¸ì •ë³´ë³´í˜¸ë²• ë˜ëŠ” ê´€ë ¨ ê·œì •(GDPR ë“±)**ì— ë”°ë¼  
            ìœ„ë°˜ ê°€ëŠ¥ì„±ì„ ë¶„ì„í•˜ê³ , ê´€ë ¨ ë²•ë ¹ê³¼ ëŒ€ì‘ ë°©ì•ˆì„ ì•ˆë‚´í•©ë‹ˆë‹¤.
        """
    }
)

################################################################################
################################ GPT UI êµ¬ì„± ###################################
################################################################################



# GPT AI êµ¬ì„±
load_dotenv()
OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')
client = OpenAI(api_key=OPENAI_API_KEY)



# 3. Streamlit UI êµ¬ì„±
st.title("ê°œì¸ì •ë³´ ë²•ë¥  ìœ í˜• ë¶„ë¥˜ ì±—ë´‡")

if 'openai_model' not in st.session_state:
    st.session_state.openai_model = 'gpt-4.1'

if 'messages' not in st.session_state:
    st.session_state.messages = []


# ê¸°ì¡´ ë©”ì‹œì§€ ì¶œë ¥
for i, msg in enumerate(st.session_state.messages):
    if msg["role"] == "system":
        continue  
    with st.chat_message(msg['role']):
        st.markdown(msg['content'])
        if msg["role"] == "assistant":
            feedback = msg.get("feedback", None)
            st.session_state[f"feedback_{i}"] = feedback
            st.feedback(
                "thumbs",
                key=f"feedback_{i}",
                disabled=feedback is not None,
                on_change=save_feedback,
                args=[i],
            )



if prompt := st.chat_input("ê°œì¸ì •ë³´ ë³´í˜¸ì— ê´€í•œ ë²•ë¥  ë¬¸ì œë¥¼ ì…ë ¥í•˜ì„¸ìš”"):

    st.session_state.messages.append({"role": "user", "content": prompt})
    
    with st.chat_message("user"):
        st.markdown(prompt)

    predicted_label = classify_legal_issue(prompt)

    with st.chat_message("assistant"):
        st.markdown(f"ğŸ” **ì˜ˆì¸¡ëœ ìœ í˜•:** {predicted_label}")
        st.markdown("AI ìƒë‹´ ë‚´ìš©ì„ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ì…ë‹ˆë‹¤...")

        gpt_prompt = f"""
        ë‹¤ìŒì€ ê°œì¸ì •ë³´ ì¹¨í•´ì™€ ê´€ë ¨ëœ ì‚¬ìš©ì ì§ˆë¬¸ì…ë‹ˆë‹¤:
        ì‚¬ìš©ì ì§ˆë¬¸: "{prompt}"
        ì˜ˆì¸¡ëœ ë²• ìœ„ë°˜ ìœ í˜•: {predicted_label}
        ì´ì— ë”°ë¼ ê°€ëŠ¥í•œ ë²•ì  ì„¤ëª…ê³¼ ëŒ€ì‘ ë°©ë²•ì„ ì•ˆë‚´í•´ ì£¼ì„¸ìš”.
        """

        stream = client.chat.completions.create(
            model=st.session_state.openai_model,
            messages=[
                {"role": "system", "content": "ë‹¹ì‹ ì€ ê°œì¸ì •ë³´ë³´í˜¸ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
                {"role": "user", "content": gpt_prompt}
            ],
            stream=True
        )

        response = st.write_stream(stream)

        st.feedback(
            "thumbs",
            key=f"feedback_{len(st.session_state.messages)}",
            on_change=save_feedback,
            args=[len(st.session_state.messages)]
        )

    
    st.session_state.messages.append({"role": "assistant", "content": response})

################################################################################
################################[ ì‚¬ ì´ ë“œ ë°” ]#################################
################################################################################

st.sidebar.empty() # ì‚¬ì´ë“œë°” ì´ˆê¸°í™”
with st.sidebar:
    # ìƒë‹¨ ì´ë¯¸ì§€ 
    st.image('https://www.kamco.or.kr/portal/img/sub/01/emot1_new_001.png')

    # FAQ SelectBox
    violation_type = st.selectbox(
        'ğŸ“Œ ë¶„ë¥˜ ì„ íƒ',
        (
            '1. ì´ˆìƒê¶Œ ì¹¨í•´',
            '2. ë™ì˜ ì—†ëŠ” ê°œì¸ì •ë³´ ìˆ˜ì§‘',
            '3. ëª©ì  ì™¸ ì´ìš©',
            '4. ì œ3ì ë¬´ë‹¨ ì œê³µ',
            '5. CCTV ê³¼ì‰ì´¬ì˜',
            '6. ì •ë³´ ìœ ì¶œ',
            '7. íŒŒê¸° ë¯¸ì´í–‰',
            '8. ê´‘ê³ ì„± ì •ë³´ ìˆ˜ì‹ ',
            '9. ê³„ì • ê´€ë ¨ ë¬¸ì œ',
            '10. ì •ë³´ì—´ëŒ/ì² íšŒê¶Œ ê±°ë¶€'
        )
    )

    st.markdown(f"ì„ íƒí•œ ìœ í˜•: **{violation_type}**")

