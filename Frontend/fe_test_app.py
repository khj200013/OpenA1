# í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬
import os
import sys
import time

# ì™¸ë¶€ ë¼ì´ë¸ŒëŸ¬ë¦¬
import streamlit as st

# sys.path ì¶”ê°€(OpenA1\Main í´ë”)
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..', 'Main')))

# ë‚´ë¶€ ëª¨ë“ˆ
## ì‚¬ì´ë“œë°”
from sidebar import render_history_sidebar, LABELS

## FileSearch ëª¨ë“ˆ
from file_search_utils import file_search_query, create_file, create_vector_store

## ìœ í˜• ë¶„ë¥˜ ëª¨ë¸ ê´€ë ¨ ëª¨ë“ˆ
from model_Frontend_v3 import classify_legal_issue, load_model

## UIë Œë” ê´€ë ¨ ëª¨ë“ˆ
from ui_components import (
    render_user_input,
    render_user_message,
    render_assistant_message,
    render_assistant_message_stream,
    render_title_image,
    render_law_info,
)

## GPT(AI) ì—°ë™ ê´€ë ¨ ëª¨ë“ˆ
## law_info_ExceptDB -> GPTê°€ ê´€ë ¨ ë²• ì¡°í•­ì„ ì°¾ê³  ë‚´ìš© ìš”ì•½, ìœ„ë°˜ ê°€ëŠ¥ì„± ì„¤ëª…
## action_guide -> ìœ„ë°˜ëœ ë²• ì¡°í•­ê³¼ ê´€ë ¨í•´ ëŒ€ì‘ ì ˆì°¨ ì•ˆë‚´
from action_guide import action_guide_agent
from law_info_ExceptDB import get_law_info
from openai_utils import get_openai_client

# JSON í•„í„°ë§ ìœ í‹¸ ì„í¬íŠ¸ ì¶”ê°€
from json_filtering import load_cases, filter_cases

# Client ì„¤ì •
client = get_openai_client()

################################################################################
################################ Page Config ###################################
################################################################################


# Page Config êµ¬ì„±
st.set_page_config(
    page_title="Streamly - An Intelligent Streamlit Assistant",
    page_icon="imgs/avatar_streamly.png",
    layout="wide",
    initial_sidebar_state="auto",
    menu_items={
        "Get Help": "https://github.com/khj200013/OpenA1/issues",
        "Report a bug": "https://github.com/khj200013/OpenA1/issues",
        "About": """
            ## AIë¥¼ í†µí•´ ê°œì¸ì •ë³´ ë³´í˜¸ë²•ì— ìœ„ë°°ë˜ëŠ” ì‚¬í•­ í™•ì¸
            ### Powered using GPT-4

            **GitHub**: https://github.com/khj200013/OpenA1
            **ë¬¸ì˜ ë˜ëŠ” ë²„ê·¸ ì œë³´**: [Issue ë“±ë¡í•˜ê¸°](https://github.com/khj200013/OpenA1/issues)

            ì´ AI ì–´ì‹œìŠ¤í„´íŠ¸ëŠ” ì‚¬ìš©ìì˜ ìƒí™©ì´ë‚˜ ì§ˆë¬¸ì„ ë°”íƒ•ìœ¼ë¡œ  
            **ê°œì¸ì •ë³´ë³´í˜¸ë²• ë˜ëŠ” ê´€ë ¨ ê·œì •(GDPR ë“±)**ì— ë”°ë¼  
            ìœ„ë°˜ ê°€ëŠ¥ì„±ì„ ë¶„ì„í•˜ê³ , ê´€ë ¨ ë²•ë ¹ê³¼ ëŒ€ì‘ ë°©ì•ˆì„ ì•ˆë‚´í•©ë‹ˆë‹¤.

            Image created by OpenAI
        """
    }
)


################################################################################
################################ GPT UI êµ¬ì„± ###################################
################################################################################


# ë¡œê³  ì´ë¯¸ì§€ ì¶œë ¥
render_title_image()

# íˆìŠ¤í† ë¦¬ìš© ë”•ì…”ë„ˆë¦¬
if "history" not in st.session_state:
    st.session_state.history = { label: [] for label in LABELS }

# JSON ì¼€ì´ìŠ¤ ë°ì´í„° ë¡œë“œ (í•œ ë²ˆë§Œ)
cases = load_cases()

# ì§ˆë¬¸â†’ë‹µë³€ ìƒì„± ë¡œì§
if "user_input" in st.session_state:
    prompt = st.session_state.user_input

# ëª¨ë¸ ë¡œë”©
if "model_loaded" not in st.session_state:
    with st.spinner("ëª¨ë¸ ë° ë²¡í„° ìŠ¤í† ì–´ë¥¼ ì´ˆê¸°í™”í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤...ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”.."):
        tokenizer, model = load_model()
        st.session_state.tokenizer = tokenizer
        st.session_state.model = model
        st.session_state.model_loaded = True

        # File Searchìš© ì„¤ì •
        file_path = "./ê°œì¸ì •ë³´ë³´í˜¸ë²•.pdf"
        file_id = create_file(client, file_path)
        vector_store_id = create_vector_store(client, file_id)

        # ì„¤ì • ë‚´ìš© session_stateì— ì €ì¥
        st.session_state.file_id = file_id
        st.session_state.vector_store = vector_store_id

if 'openai_model' not in st.session_state:
    st.session_state.openai_model = 'gpt-4.1'

if 'messages' not in st.session_state:
    st.session_state.messages = []


# ê¸°ì¡´ ë©”ì‹œì§€ ì¶œë ¥
for i, msg in enumerate(st.session_state.messages):
    if msg["role"] == "system":
        continue  
    if msg["role"] == 'user':
        render_user_message(msg["content"])
    elif  msg["role"] == 'assistant': 
        if msg["law_info"]:
            render_law_info(msg["law_info"], msg["file_search_result"])
        render_assistant_message(msg["predicted_label"], msg["content"])


# ì‚¬ìš©ì ì…ë ¥ ì €ì¥ í›„ rerun ì²˜ë¦¬ (stream ì¤‘ ì…ë ¥ ë°©ì§€)
if "user_input" not in st.session_state:
    if prompt := render_user_input():
        st.session_state.user_input = prompt
        st.rerun()

# GPT ì‘ë‹µ ì²˜ë¦¬
if "user_input" in st.session_state:
    prompt = st.session_state.user_input
    # ì‚¬ìš©ì ë©”ì‹œì§€ session_stateì— ì¶”ê°€
    st.session_state.messages.append({"role": "user", "content": prompt})

    # ìœ ì € ë©”ì‹œì§€ ë Œë”
    render_user_message(prompt)

    predict_start_time = time.time()

    # --------------------------
    # 1ì°¨ JSON í•„í„°ë§ ì‹œë„
    matched_case = filter_cases(prompt, cases)

    if matched_case is not None:
        # JSON 1ì°¨ í•„í„°ë§ ê²°ê³¼ ì¶œë ¥ ë° íˆìŠ¤í† ë¦¬ ì €ì¥
        predicted_label = matched_case.get("label", "ì•Œ ìˆ˜ ì—†ìŒ")
        law_article = matched_case.get("law_article", "")

    else:
        # JSON ë§¤ì¹­ ì•ˆë˜ë©´ ê¸°ì¡´ ë¶„ë¥˜ + GPT ì²˜ë¦¬
        predicted_label = classify_legal_issue(prompt, st.session_state.tokenizer, st.session_state.model)
        law_article = None

    predict_end_time = time.time()

    print(f'ë¼ë²¨ ë¶„ë¥˜ ê±¸ë¦° ì‹œê°„ :::: {predict_end_time-predict_start_time:.2f}ì´ˆ')

    # GPT PROMPT
    with st.spinner("ğŸ” ë²•ë¥  ì •ë³´ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤..."):
        law_info_start_time = time.time()


        law_info = get_law_info(predicted_label, prompt)

            
        law_info__end_time = time.time()

        print(f'ë²•ë¥ ì •ë³´ ë¶„ì„ ê±¸ë¦° ì‹œê°„ :::: {law_info__end_time-law_info_start_time:.2f}ì´ˆ')

        file_search_start_time = time.time()

        # File Search
        file_search_result = file_search_query(client, law_info['law'], st.session_state.vector_store)

        
        file_search__end_time = time.time()

        print(f'íŒŒì¼ì„œì¹˜ ê±¸ë¦° ì‹œê°„ :::: {file_search__end_time-file_search_start_time:.2f}ì´ˆ')

    # Law info + File search ê°’ ì¶œë ¥
    render_law_info(law_info, file_search_result)

    response_stream = action_guide_agent(prompt, predicted_label)

    # AI ë©”ì‹œì§€ ë Œë” (Stream í˜•íƒœ)
    response_text = render_assistant_message_stream(predicted_label, response_stream)
    
    # AI ë©”ì‹œì§€ session_stateì— ì¶”ê°€
    st.session_state.messages.append({"role": "assistant", 
                                      "predicted_label": predicted_label, 
                                      "law_info" : law_info,
                                      "file_search_result" : file_search_result,
                                      "content": response_text})
    # ì§ˆë¬¸ + ë‹µë³€ ì €ì¥
    st.session_state.history[predicted_label].append({
        "question": prompt,
        "answer": response_text
    })
    
    # user_input ì‚­ì œ í›„ rerun
    del st.session_state.user_input
    st.rerun()

################################################################################
################################[ ì‚¬ ì´ ë“œ ë°” ]#################################
################################################################################

render_history_sidebar()

