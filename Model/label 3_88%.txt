# ğŸ’¡ ì„¤ì¹˜ í•„ìš” ì‹œ (ì£¼ì„ í•´ì œ)
!pip install transformers datasets

import pandas as pd
from sklearn.model_selection import train_test_split
from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments
import torch
from torch.utils.data import Dataset
import numpy as np
from datasets import load_metric
from sklearn.metrics import confusion_matrix, classification_report

# 1. ë°ì´í„° ë¡œë“œ (privacy_legal_data.csv ì‚¬ìš©)
data = [
    ("íšŒì‚¬ì—ì„œ ë‚´ ì–¼êµ´ ì‚¬ì§„ì„ í—ˆë½ ì—†ì´ í™ë³´ì— ì‚¬ìš©í–ˆì–´ìš”",0),
    ("ë‚´ ì‚¬ì§„ì´ ë¬´ë‹¨ìœ¼ë¡œ SNSì— ì˜¬ë¼ê°”ì–´ìš”",0),
    ("í™ë³´ìš© í¬ìŠ¤í„°ì— ì œ ì–¼êµ´ì´ ë™ì˜ ì—†ì´ ì‚¬ìš©ë˜ì—ˆì–´ìš”",0),
    ("ë™ë£Œê°€ ì œ ì‚¬ì§„ì„ ëª°ë˜ ì°ì–´ì„œ ì¸í„°ë„·ì— ì˜¬ë ¸ìŠµë‹ˆë‹¤",0),
    ("ì§€í•˜ì² ì—ì„œ ëª°ë˜ ì‚¬ì§„ ì°í˜”ìŠµë‹ˆë‹¤",0),
    ("í•™ì›ì—ì„œ ì œ ì‚¬ì§„ì„ í—ˆë½ ì—†ì´ ì›¹ì‚¬ì´íŠ¸ì— ê²Œì‹œí–ˆì–´ìš”",0),
    ("ì•„ë¬´ëŸ° ë™ì˜ ì—†ì´ ì˜ìƒ ì´¬ì˜ì„ ë‹¹í–ˆì–´ìš”",0),
    ("ë‚´ ì–¼êµ´ì´ ê´‘ê³ ì— ì‚¬ìš©ëëŠ”ë° ì‚¬ì „ ë™ì˜ê°€ ì—†ì—ˆì–´ìš”",0),
    ("í•™êµ í–‰ì‚¬ ì‚¬ì§„ì— ì œ ì–¼êµ´ì´ ë‚˜ì™”ëŠ”ë° ë™ì˜í•œ ì  ì—†ìŠµë‹ˆë‹¤",0),
    ("ì¹´í˜ì—ì„œ ëª°ë˜ ì‚¬ì§„ ì°í˜€ì„œ ê¸°ë¶„ì´ ë‚˜ì©ë‹ˆë‹¤",0),

    ("ë‚´ ê°œì¸ì •ë³´ê°€ ìœ ì¶œëœ ê²ƒ ê°™ì•„ìš”",1),
    ("ìŠ¤íŒ¸ ì „í™”ê°€ ë„ˆë¬´ ìì£¼ ì™€ì„œ ì •ë³´ê°€ ìƒˆë‚˜ê°„ ë“¯í•©ë‹ˆë‹¤",1),
    ("íšŒì‚¬ì—ì„œ ìˆ˜ì§‘í•œ ì •ë³´ê°€ ì™¸ë¶€ì— ê³µê°œëì–´ìš”",1),
    ("ë‚´ ì£¼ì†Œê°€ ì¸í„°ë„·ì— ì˜¬ë¼ê°€ ìˆì–´ìš”",1),
    ("ì´ë©”ì¼ ì£¼ì†Œê°€ ìœ ì¶œë¼ì„œ ìŠ¤íŒ¸ì´ ìŸì•„ì§‘ë‹ˆë‹¤",1),
    ("ëˆ„êµ°ê°€ ë‚´ ì£¼ë¯¼ë“±ë¡ë²ˆí˜¸ë¥¼ ì•Œê³  ìˆì–´ìš”",1),
    ("ê°œì¸ì •ë³´ê°€ ìœ ì¶œë¼ ê¸ˆìœµì‚¬ê¸°ë¡œ ì´ì–´ì¡Œì–´ìš”",1),
    ("ì–´ë””ì„ ê°€ ë‚´ íœ´ëŒ€í° ë²ˆí˜¸ë¥¼ ì•Œì•„ëƒˆì–´ìš”",1),
    ("íšŒì‚¬ ë‚´ë¶€ ì§ì›ì´ ì •ë³´ë¥¼ ì™¸ë¶€ë¡œ ìœ ì¶œí–ˆìŠµë‹ˆë‹¤",1),
    ("ì •ë³´ê°€ í•´í‚¹ìœ¼ë¡œ ë¹ ì ¸ë‚˜ê°„ ê²ƒ ê°™ìŠµë‹ˆë‹¤",1),

    ("ë™ì˜ ì—†ì´ ê°œì¸ì •ë³´ë¥¼ ìˆ˜ì§‘í–ˆì–´ìš”",2),
    ("ì‚¬ì „ ê³ ì§€ ì—†ì´ ê°œì¸ì •ë³´ ë™ì˜ë¥¼ ë°›ì§€ ì•Šì•˜ì–´ìš”",2),
    ("ë™ì˜ì„œë„ ì—†ì´ ì œ ì •ë³´ë¥¼ ì…ë ¥ë°›ì•˜ì–´ìš”",2),
    ("íšŒì›ê°€ì… ì‹œ ê°œì¸ì •ë³´ ë™ì˜ ì ˆì°¨ê°€ ì—†ì—ˆìŠµë‹ˆë‹¤",2),
    ("ë‚´ê°€ ë™ì˜í•˜ì§€ ì•Šì€ ë§ˆì¼€íŒ… ì •ë³´ê°€ ë¬¸ìë¡œ ì˜µë‹ˆë‹¤",2),
    ("ë™ì˜í•˜ì§€ ì•Šì€ ì œ ì •ë³´ë¥¼ ì‚¬ìš©í•˜ê³  ìˆì–´ìš”",2),
    ("ì•±ì´ ë‚´ ì •ë³´ë¥¼ ìˆ˜ì§‘í•˜ëŠ”ë° ë™ì˜í•œ ì ì´ ì—†ì–´ìš”",2),
    ("ì‚¬ì´íŠ¸ì—ì„œ ë™ì˜ ì—†ì´ ë‚´ ìœ„ì¹˜ì •ë³´ë¥¼ ì¶”ì í•©ë‹ˆë‹¤",2),
    ("ë‚´ ê¸°ë¡ì„ ë™ì˜ ì—†ì´ ë¶„ì„í•˜ê³  ìˆì–´ìš”",2),
    ("ì²˜ë¦¬ ëª©ì ì´ ëª…í™•í•˜ì§€ ì•Šì€ë°ë„ ë™ì˜ë¥¼ ìš”êµ¬í•˜ì§€ ì•Šì•˜ì–´ìš”",2),

    ("ë™ì˜ ì—†ì´ CCTV ì˜ìƒì´ ìˆ˜ì§‘ë˜ì—ˆì–´ìš”",2),
    ("ë§ˆì¼€íŒ… ì •ë³´ ìˆ˜ì‹  ê±°ë¶€í–ˆëŠ”ë° ê³„ì† ì—°ë½ì´ ì™€ìš”",2),
    ("ì•±ì„ ì„¤ì¹˜í•˜ìë§ˆì ìœ„ì¹˜ì •ë³´ë¥¼ ê°€ì ¸ê°”ì–´ìš”",2),
    ("ê³µê³µê¸°ê´€ì—ì„œ ê°œì¸ì •ë³´ ìˆ˜ì§‘ ì‹œ ê³ ì§€ê°€ ì—†ì—ˆìŠµë‹ˆë‹¤",2),
    ("ê°œì¸ì •ë³´ í™œìš© ë™ì˜ ì°½ì´ ì•„ì˜ˆ ì—†ì—ˆì–´ìš”",2),
    ("íšŒì‚¬ì—ì„œ ë‚´ ë™ì˜ ì—†ì´ ì œ ì •ë³´ë¥¼ íŒŒíŠ¸ë„ˆì‚¬ì— ì œê³µí–ˆì–´ìš”",2),
    ("ì •ë³´ ì œê³µ ë™ì˜ ì ˆì°¨ê°€ íˆ¬ëª…í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤",2),
    ("ë‚´ ì •ë³´ê°€ ë§ˆì¼€íŒ…ì— í™œìš©ë˜ê³  ìˆëŠ”ë° ë™ì˜í•œ ê¸°ì–µì´ ì—†ì–´ìš”",2),
    ("ë™ì˜ ì—†ì´ ë‚´ ì •ë³´ë¥¼ ì œ3ìì—ê²Œ ë„˜ê¸´ ê²ƒ ê°™ì•„ìš”",2),
    ("ë‚´ê°€ ì œê³µí•œ ì  ì—†ëŠ” ì •ë³´ê°€ ì‹œìŠ¤í…œì— ë“±ë¡ë¼ ìˆì–´ìš”",2),
]

df = pd.DataFrame(data, columns=["text", "label"])
label_map = {0: "ì´ˆìƒê¶Œ ì¹¨í•´", 1: "ê°œì¸ì •ë³´ ìœ ì¶œ", 2: "ë™ì˜ ë¯¸ì´í–‰"}

train_texts, val_texts, train_labels, val_labels = train_test_split(
    df['text'].tolist(), df['label'].tolist(), test_size=0.2, random_state=42
)

# 2. ë°ì´í„°ì…‹ í´ë˜ìŠ¤ ì •ì˜
class LegalDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_len=64):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_len = max_len
        
    def __len__(self):
        return len(self.texts)
    
    def __getitem__(self, idx):
        text = self.texts[idx]
        label = self.labels[idx]
        encoding = self.tokenizer(
            text,
            max_length=self.max_len,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )
        return {
            'input_ids': encoding['input_ids'].squeeze(),
            'attention_mask': encoding['attention_mask'].squeeze(),
            'labels': torch.tensor(label)
        }

# 3. KLUE BERT ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
model_name = "klue/bert-base"
tokenizer = BertTokenizer.from_pretrained(model_name)
model = BertForSequenceClassification.from_pretrained(model_name, num_labels=3)

# 4. ë°ì´í„°ì…‹ ì¤€ë¹„
train_dataset = LegalDataset(train_texts, train_labels, tokenizer)
val_dataset = LegalDataset(val_texts, val_labels, tokenizer)

# 5. í‰ê°€ ì§€í‘œ ì„¤ì •
accuracy_metric = load_metric("accuracy")
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    preds = np.argmax(logits, axis=-1)
    return accuracy_metric.compute(predictions=preds, references=labels)

# 6. í›ˆë ¨ ì„¤ì •
training_args = TrainingArguments(
    output_dir='./klue_bert_results',
    num_train_epochs=10,
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    logging_steps=5,
    eval_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
    metric_for_best_model="accuracy",
    report_to="none"  # wandb ë„ê¸°
)

# 7. Trainer ê°ì²´
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    compute_metrics=compute_metrics
)

# 8. í•™ìŠµ ì‹œì‘
trainer.train()

model.eval()
all_preds = []
all_labels = []

for batch in val_dataset:
    inputs = {
        'input_ids': batch['input_ids'].unsqueeze(0),
        'attention_mask': batch['attention_mask'].unsqueeze(0)
    }
    with torch.no_grad():
        outputs = model(**inputs)
    pred = torch.argmax(outputs.logits, dim=1).item()
    all_preds.append(pred)
    all_labels.append(batch['labels'].item())

# í˜¼ë™ í–‰ë ¬ ì¶œë ¥
cm = confusion_matrix(all_labels, all_preds)
print("Confusion Matrix:")
print(cm)

# ìƒì„¸ ë¶„ë¥˜ ë³´ê³ ì„œ ì¶œë ¥
print("\nClassification Report:")
print(classification_report(all_labels, all_preds, target_names=[label_map[i] for i in range(3)]))

# âœ… 10. ëª¨ë¸ ì €ì¥ (ëª¨ë¸ + í† í¬ë‚˜ì´ì €)
model.save_pretrained("./saved_kobert_model")
tokenizer.save_pretrained("./saved_kobert_model")

# 9. ì¶”ë¡  í•¨ìˆ˜
def classify_legal_issue(text):
    model.eval()
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=64)
    with torch.no_grad():
        outputs = model(**inputs)
    pred = torch.argmax(outputs.logits, dim=1).item()
    return label_map[pred]

# 10. ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸
test_sentence = "íšŒì‚¬ì—ì„œ ë‚´ ì–¼êµ´ ì‚¬ì§„ì„ í—ˆë½ ì—†ì´ í™ë³´ì— ì¼ìŒ"
print(f"ì…ë ¥: {test_sentence}")
print(f"ì˜ˆì¸¡ëœ ìœ í˜•: {classify_legal_issue(test_sentence)}")
